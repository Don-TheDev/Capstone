{
  "mood_lonely": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 19,
    "confused_with": {}
  },
  "mood_great": {
    "precision": 0.7647058823529411,
    "recall": 0.9285714285714286,
    "f1-score": 0.8387096774193549,
    "support": 14,
    "confused_with": {
      "mood_happy": 1
    }
  },
  "greet_smalltalk": {
    "precision": 0.9166666666666666,
    "recall": 0.7857142857142857,
    "f1-score": 0.8461538461538461,
    "support": 14,
    "confused_with": {
      "greet": 3
    }
  },
  "temperature_weather": {
    "precision": 0.9259259259259259,
    "recall": 0.9615384615384616,
    "f1-score": 0.9433962264150944,
    "support": 26,
    "confused_with": {
      "type_weather": 1
    }
  },
  "give_city": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 5,
    "confused_with": {}
  },
  "Computer_Science_Courses": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 15,
    "confused_with": {}
  },
  "mood_angry": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 32,
    "confused_with": {}
  },
  "goodbye": {
    "precision": 0.7857142857142857,
    "recall": 0.9166666666666666,
    "f1-score": 0.8461538461538461,
    "support": 12,
    "confused_with": {
      "goodbyes_smalltalk": 1
    }
  },
  "mood_happy": {
    "precision": 0.9629629629629629,
    "recall": 0.9285714285714286,
    "f1-score": 0.9454545454545454,
    "support": 28,
    "confused_with": {
      "mood_great": 2
    }
  },
  "mood_scared": {
    "precision": 0.95,
    "recall": 0.95,
    "f1-score": 0.9500000000000001,
    "support": 20,
    "confused_with": {
      "mood_nervous": 1
    }
  },
  "mood_nervous": {
    "precision": 0.9767441860465116,
    "recall": 0.9767441860465116,
    "f1-score": 0.9767441860465116,
    "support": 43,
    "confused_with": {
      "mood_scared": 1
    }
  },
  "mood_tired": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 20,
    "confused_with": {}
  },
  "greet": {
    "precision": 0.7857142857142857,
    "recall": 0.8461538461538461,
    "f1-score": 0.8148148148148148,
    "support": 13,
    "confused_with": {
      "goodbye": 1,
      "greet_smalltalk": 1
    }
  },
  "affirm_smalltalk": {
    "precision": 0.7692307692307693,
    "recall": 0.8333333333333334,
    "f1-score": 0.8,
    "support": 12,
    "confused_with": {
      "mood_great": 2
    }
  },
  "mood_unhappy": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 61,
    "confused_with": {}
  },
  "ask_forecast_weather": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 56,
    "confused_with": {}
  },
  "deny_smalltalk": {
    "precision": 0.6111111111111112,
    "recall": 0.7333333333333333,
    "f1-score": 0.6666666666666666,
    "support": 15,
    "confused_with": {
      "deny": 3,
      "bot_challenge": 1
    }
  },
  "mood_bored": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 20,
    "confused_with": {}
  },
  "mood_excited": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 12,
    "confused_with": {}
  },
  "Literature_Courses": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6,
    "confused_with": {}
  },
  "bot_challenge": {
    "precision": 0.5,
    "recall": 0.125,
    "f1-score": 0.2,
    "support": 8,
    "confused_with": {
      "deny_smalltalk": 7
    }
  },
  "deny": {
    "precision": 0.7,
    "recall": 1.0,
    "f1-score": 0.8235294117647058,
    "support": 7,
    "confused_with": {}
  },
  "affirm": {
    "precision": 1.0,
    "recall": 0.5,
    "f1-score": 0.6666666666666666,
    "support": 6,
    "confused_with": {
      "affirm_smalltalk": 3
    }
  },
  "Management_Courses": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 11,
    "confused_with": {}
  },
  "goodbyes_smalltalk": {
    "precision": 0.8,
    "recall": 0.6666666666666666,
    "f1-score": 0.7272727272727272,
    "support": 6,
    "confused_with": {
      "goodbye": 2
    }
  },
  "type_weather": {
    "precision": 0.9919354838709677,
    "recall": 0.984,
    "f1-score": 0.9879518072289156,
    "support": 125,
    "confused_with": {
      "temperature_weather": 2
    }
  },
  "accuracy": 0.9471947194719472,
  "macro avg": {
    "precision": 0.9015658292152472,
    "recall": 0.8898574475613833,
    "f1-score": 0.8859044008483731,
    "support": 606
  },
  "weighted avg": {
    "precision": 0.9477037644347782,
    "recall": 0.9471947194719472,
    "f1-score": 0.9441549098772064,
    "support": 606
  }
}